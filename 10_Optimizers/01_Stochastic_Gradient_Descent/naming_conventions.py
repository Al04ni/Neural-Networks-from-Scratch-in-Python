
# Stochastic Gradient Descent (SGD):
# - Historically refers to an optimizer that fits a single sample at a time.

# Batch Gradient Descent (BGD):
# - Optimizer used to fit a whole dataset at once.

# Mini-batch Gradient Descent (MBGD):
# - Used to fit slices of a dataset, which we'd call batches in our context (this book).